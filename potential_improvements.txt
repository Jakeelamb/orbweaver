Optimizing Grammar Construction for Longer, Lower Quantity Rules:
The core of Sequitur-like algorithms is iterative: find the most frequent pair (or k-mer) and replace it with a rule. When rules become longer (hierarchically) and less frequent, the process can slow down if:
Discovering these longer/rarer patterns is inefficient.
The definition of "most frequent" doesn't adapt well to prioritize structurally significant longer patterns.
Here are some recommendations to improve performance in these scenarios:
Enhanced Pattern Discovery with Suffix Structures:
Generalized Suffix Trees (GSTs) or Suffix Automata: Currently, you use suffix arrays for initial terminal digram finding. Consider employing GSTs or Suffix Automata on the working sequence (which includes non-terminals) periodically. These structures can identify all maximal repeats (potential rules of variable length) and their frequencies more directly than iterating on fixed-size k-mers. This is powerful for finding "longer" rules that might not be the absolute most frequent digram/k-mer but offer better compression.
Integration Strategy: You could use your current k-mer approach for initial compression. When the rate of finding new high-frequency k-mers drops, switch to a GST-based search for longer, sparser rules.
The main challenge here is the complexity of building and updating these structures, especially on a sequence that changes. However, rebuilding them periodically might still be faster than many slow iterations of k-mer searching.
Rule Scoring and Prioritization Heuristics:
Instead of picking purely the "most frequent" pattern, implement a rule scoring system. A common heuristic is score = frequency * (length_of_pattern - 1).
This score gives more weight to longer patterns, which can lead to faster convergence and better overall grammar quality, even if a longer pattern's raw frequency is slightly lower than a very short, common one.
When a pattern involves non-terminals (e.g., R1 A or R1 R2), length_of_pattern would be the sum of the expanded lengths of its constituent symbols. Calculating this on-the-fly for all candidates can be intensive, so you might need to cache expanded lengths of rules or use estimations.
This would involve modifying find_most_frequent_digram in DigramTable and find_most_frequent_kmer in KmerTable to return the highest scoring pattern according to your new heuristic.
Optimizing KmerTable Operations (if k-mer size > 2 is used):
The KmerTable::find_most_frequent_kmer method iterates through the entire occurrences HashMap to find the k-mer with the maximum count. If this map becomes very large (many unique k-mers, each with relatively low frequency in later stages), this linear scan is inefficient.
Consider maintaining an auxiliary data structure that keeps k-mers partially sorted by frequency, or a min-priority queue holding the top N most frequent/highest-scoring k-mer candidates. This would make querying faster, though updates to the table would be more complex.
Adaptive Strategies and Parallelism for Later Stages:
Adaptive K-mer Size: Consider strategies where the kmer_size might adapt during the grammar construction process. As the sequence gets compressed and average rule length increases, a larger k-mer size might become more effective.
Batching Rule Replacements: If multiple non-overlapping, high-scoring patterns are found, replacing them in a single pass before rebuilding the pattern tables (DigramTable, KmerTable) could reduce the overhead of table reconstruction.
Review Parallelism: Ensure that parallel table building (DigramTable::build_parallel) remains effective as the sequence becomes more heterogeneous with many non-terminals. The characteristics of the data change, which might affect load balancing.
Profile-Guided Optimization (Reiteration):
Continue to use profiling tools like cargo flamegraph on datasets where the slowdown for longer, lower-quantity rules is apparent. This will help confirm if the bottlenecks are in pattern discovery (table building, searching) or elsewhere in the later stages of grammar construction.
These strategies involve different levels of implementation complexity. Starting with rule scoring (2) might be the most straightforward to implement and could yield good results. Enhanced pattern discovery (1) is more involved but potentially offers the most significant speed-up for the specific problem of finding longer rules.

Workflow Management and Operational Features:
The tool needs robust mechanisms for handling multiple genome assembly runs, organizing outputs, and resuming interrupted processes.

1.  **Output Organization:**
    *   **Standardized Directory Structure:** Implement a clear and configurable directory structure for storing outputs. For example: `[base_output_dir]/[species_id]/[assembly_id]/[run_timestamp_or_id]/`. This ensures outputs are easily locatable and segregated.
    *   **Configuration:** Allow users to specify the `base_output_dir`.
    *   **Metadata Logging:** Alongside the primary results (e.g., constructed grammar), store metadata for each run. This should include:
        *   Input file paths/identifiers.
        *   Parameters used for the run.
        *   Start time, end time, and duration.
        *   Completion status (`completed`, `failed`, `in_progress`).
        *   Version of the tool used.
        *   Log files for diagnostics.
    *   Consider using a simple manifest file (e.g., JSON or YAML) within each run directory to store this metadata.

2.  **Run Resumption for Failed or Stopped Jobs:**
    *   **Checkpointing:** Introduce a checkpointing mechanism to periodically save the intermediate state of the grammar construction process.
        *   Identify critical data structures and variables that define the state (e.g., current sequence, rule set, k-mer tables, progress counters).
        *   Determine optimal checkpointing frequency (e.g., after processing a certain number of input sequences, after a specific number of iterations, or time-based).
        *   Store checkpoints reliably, potentially in the run's output directory. Ensure checkpoints are named or versioned to allow rollback if needed.
    *   **State Management/Tracking:** Maintain a persistent record of the status of each assembly being processed.
        *   This could be a simple file-based ledger (e.g., a CSV or SQLite database at the `base_output_dir` level) that tracks each assembly, its last known status, and the path to its latest checkpoint.
    *   **Resumption Logic:**
        *   When the tool is invoked for an assembly, it should first check its status.
        *   If a previous run for that assembly was marked as `failed` or was `in_progress` (and a valid checkpoint exists), the tool should offer (or automatically attempt) to resume from the last successful checkpoint.
        *   This involves loading the saved state and continuing the processing from that point.
    *   **Idempotency:** Design the processing steps to be as idempotent as possible, so that re-running a completed step from a checkpoint does not produce incorrect results.
    *   **Error Handling:** Robustly handle errors during checkpoint saving and loading. If a checkpoint is corrupted, the tool might need to restart from an earlier checkpoint or from scratch.

Let me know if you'd like to delve deeper into any of these specific areas or want to discuss how to approach their implementation!